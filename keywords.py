import os
import logging
from enum import Enum


SUBSCRIBED_TOPICS = ['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG', 'cs.RO', 'cs.DC', 'cs.AR']
SAVE_DIR = 'papers_data'
IMPORTANT_ORGS = ['MIT', 'Stanford', 'Google Research']

ARXIV_API_URL = 'http://export.arxiv.org/api/query'

if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class Conference(Enum):
    ARXIV = "arxiv"
    MLSYS2025 = "mlsys2025"
    ISCA2025 = "isca2025"
    ICRA2025 = "icra2025"


researcher_names_only = [
    "John Schulman",
    "Lilian Weng",
    "Long Ouyang",
    "Alec Radford",
    "Wojciech Zaremba",
    "Mark Chen",
    "Aditya Ramesh",
    "Tim Brooks",
    "Bill Peebles",
    "Prafulla Dhariwal",
    "Sam Altman",
    "Greg Brockman",
    "Ilya Sutskever",
    "Demis Hassabis",
    "Shane Legg",
    "David Silver",
    "Oriol Vinyals",
    "Jeff Dean",
    "Nando de Freitas",
    "Raia Hadsell",
    "Volodymyr Mnih",
    "Pushmeet Kohli",
    "Aakanksha Chowdhery",
    "Zoubin Ghahramani",
    "Doina Precup",
    "Koray Kavukcuoglu",
    "Jean-Baptiste Alayrac",
    "Slav Petrov",
    "Norm Jouppi",
    "Cliff Young",
    "Jonathan T. Barron",
    "Ricardo Martin-Brualla",
    "Matthew Tancik",
    "Chao Jia",
    "Cordelia Schmid",
    "Vincent Vanhoucke",
    "Carolina Parada",
    "Yann LeCun",
    "Joelle Pineau",
    "Jason Weston",
    "Armand Joulin",
    "Thomas Scialom",
    "Timothée Lacroix",
    "Laurens van der Maaten",
    "Georgia Gkioxari",
    "David Novotny",
    "Devi Parikh",
    "Dario Amodei",
    "Jared Kaplan",
    "Amanda Askell",
    "Yuntao Bai",
    "Jan Leike",
    "Paul Christiano",
    "Johannes Gehrke",
    "Eric Horvitz",
    "Peter Lee",
    "Sébastien Bubeck",
    "Ranveer Chandra",
    "Evelyne Viegas",
    "Jianfeng Gao",
    "Linjie Li",
    "Percy Liang",
    "Chelsea Finn",
    "Dorsa Sadigh",
    "Chris Ré",
    "Fei-Fei Li",
    "Jure Leskovec",
    "Jiajun Wu",
    "Silvio Savarese",
    "Kunle Olukotun",
    "Matei Zaharia",
    "Gordon Wetzstein",
    "Leonidas Guibas",
    "Jeannette Bohg",
    "Ranjay Krishna",
    "Christopher Manning",
    "Pieter Abbeel",
    "Sergey Levine",
    "Michael I. Jordan",
    "Stuart Russell",
    "Dawn Song",
    "Alexei (Alyosha) Efros",
    "Jitendra Malik",
    "Angjoo Kanazawa",
    "Trevor Darrell",
    "Krste Asanović",
    "David Patterson",
    "Ion Stoica",
    "Ben Mildenhall",
    "Ren Ng",
    "Anca Dragan",
    "Ken Goldberg",
    "Ruslan Salakhutdinov",
    "Louis-Philippe Morency",
    "Eric P. Xing",
    "Graham Neubig",
    "Abhinav Gupta",
    "Kris Kitani",
    "Onur Mutlu",
    "Todd Mowry",
    "Drew Bagnell",
    "Oliver Kroemer",
    "Martial Hebert",
    "Alexander Hauptmann",
    "Josh Tenenbaum",
    "Leslie Kaelbling",
    "Armando Solar-Lezama",
    "Jacob Andreas",
    "Antonio Torralba",
    "Russ Tedrake",
    "Bill Freeman",
    "Vivienne Sze",
    "Joel Emer",
    "Daniela Rus",
    "Sangbae Kim",
    "Vincent Sitzmann",
    "Aude Oliva",
    "Yejin Choi",
    "Hannaneh Hajishirzi",
    "Ali Farhadi",
    "Luke Zettlemoyer",
    "Siddhartha Srinivasa",
    "Dieter Fox",
    "Luis Ceze",
    "Geoffrey Hinton",
    "Yoshua Bengio",
    "Jimmy Ba",
    "Roger Grosse",
    "Raquel Urtasun",
    "Aaron Courville",
    "Clement Delangue",
    "Julien Chaumond",
    "Thomas Wolf",
    "Sam Shleifer",
    "Douwe Kiela",
    "Oren Etzioni",
    "Peter Clark",
    "Jensen Huang",
    "Bill Dally",
    "Bryan Catanzaro",
    "Anima Anandkumar",
    "Jonah Alben",
    "Sanja Fidler",
    "Tero Karras",
    "Alexander Bergman",
    "Fabio Ramos",
    "Lisa Su",
    "Andrew Feldman",
    "Peter Chen",
    "Cristóbal Valenzuela",
    "Emad Mostaque",
    "Robin Rombach",
    "Torsten Hoefler",
    "Wen-mei Hwu",
    "Marc Pollefeys",
    "Fisher Yu",
    "Michael J. Black",
    "Kyunghyun Cho",
    "Yoav Goldberg",
    "Jie Tang",
    "Jun Zhu",
    "Minlie Huang",
    "Maosong Sun",
    "Ming Zhou",
    "Bin Cui",
    "Qiang Yang"
]

# To ensure uniqueness if any subtle duplicates were introduced by stripping affiliations
unique_researcher_names_only = sorted(list(set(researcher_names_only)))


final_split_keywords_list = [
    "3D GANs", # from "Generative Adversarial Networks for 3D (3D GANs)"
    "3D Generation",
    "3D Gaussian Splatting",
    "3D Reconstruction",
    "3D Scene Understanding",
    "3D Synthesis",
    "A2C", # from "Actor-Critic / A2C / A3C / PPO / SAC / DDPG"
    "A3C", # from "Actor-Critic / A2C / A3C / PPO / SAC / DDPG"
    "AI Accelerator(s)",
    "AI Agent(s)",
    "AI Alignment",
    "AI Chip(s)",
    "AI for Robotics",
    "AI Safety",
    "ALBERT", # from "BERT / RoBERTa / ALBERT"
    "ASIC (Application-Specific Integrated Circuit) for AI",
    "Actor-Critic",
    "Adam", # from "Optimization / Stochastic Gradient Descent (SGD) / Adam"
    "Alignment",
    "AlphaGo", 
    "AlphaZero", 
    "Audio-Visual Learning",
    "Autonomous Agent(s)",
    "Avatar Generation",
    "BERT",
    "Batch RL",
    "Benchmarking",
    "Bias",
    "CLIP (Contrastive Language–Image Pre-training)",
    "CUDA",
    "Chain-of-Thought (CoT)",
    "Checkpointing",
    "Chiplet",
    "Claude",
    "Code Generation",
    "Constitutional AI",
    "Control",
    "Conversational AI",
    "Cross-modal Learning",
    "Curiosity",
    "DALL-E",
    "DDPG", # from "Actor-Critic / A2C / A3C / PPO / SAC / DDPG"
    "DQN", # from "Value Function / Q-Learning / DQN"
    "DPO", # from "Direct Preference Optimization (DPO) / Identity Preference Optimisation (IPO)"
    "Decision Making (with LLMs)",
    "Deep Learning",
    "Dialogue Systems",
    "Diffusion Models for 3D",
    "Digital Human(s)",
    "Direct Preference Optimization (DPO)",
    "Disentanglement",
    "Distributed Training",
    "Domain Adaptation",
    "Efficient Inference",
    "Efficient Training",
    "Embodied AI",
    "Embodied Agent(s)",
    "Evaluation",
    "Explainability (XAI)",
    "Exploitation",
    "Exploration",
    "FP8 Quantization",
    "FPGA (Field-Programmable Gate Array) for AI",
    "Factual Accuracy",
    "Fairness",
    "Fast Inference",
    "Federated Learning",
    "Few-Shot Learning",
    "Flamingo",
    "Flan", # from "PaLM / Gemini / Flan"
    "Foundation Model(s)",
    "Foundation Models for Robotics",
    "GPT",
    "GPT-3",
    "GPT-4",
    "GPT-4V",
    "GPT-N",
    "GPU",
    "GTC (Graph of Thoughts)", # Assuming GoT means Graph of Thoughts
    "Gemini",
    "Gen-2",
    "Generalization",
    "Generative AI",
    "Generative Adversarial Networks for 3D (3D GANs)",
    "Generative Model(s)",
    "Gradient Accumulation",
    "Graph of Thoughts (GoT)",
    "Grasping",
    "Graphics Processing Unit",
    "Grounded Language Learning",
    "HRI", # from "Human-Robot Interaction (HRI)"
    "HRL", # from "Hierarchical Reinforcement Learning (HRL)"
    "Hallucination",
    "Hardware Accelerator(s)",
    "Heterogeneous Integration",
    "Hierarchical Reinforcement Learning (HRL)",
    "Human-Robot Interaction (HRI)",
    "Human-in-the-Loop RL",
    "ICL", # from "In-Context Learning (ICL)"
    "INRs", # from "Volumetric Rendering / Implicit Neural Representations (INRs)"
    "INT8 Quantization",
    "IPO", # from "Direct Preference Optimization (DPO) / Identity Preference Optimisation (IPO)"
    "IRL", # from "Imitation Learning / Inverse Reinforcement Learning (IRL)"
    "Identity Preference Optimisation (IPO)",
    "Image Captioning",
    "Image-to-3D",
    "Imagen",
    "Imitation Learning",
    "Imitation Learning for Robotics",
    "Implicit Neural Representations (INRs)",
    "In-Context Learning (ICL)",
    "In-Memory Computing",
    "Inference Acceleration",
    "Instant-NGP",
    "Instruction Following",
    "Instruction Tuning",
    "Intelligent Agent(s)",
    "Intelligent Robotics",
    "Interactive Learning",
    "Interpretability",
    "Intrinsic Motivation",
    "Inverse Reinforcement Learning (IRL)",
    "KOSMOS",
    "Knowledge Distillation",
    "Knowledge Retrieval",
    "LLM Inference",
    "LLM Agent(s)",
    "LLM for RL",
    "LLM for Robotics",
    "LLM", # from "Large Language Model(s) / LLM(s)"
    "LLaMA",
    "LMMs", # from "Multimodal LLMs / Large Multimodal Models (LMMs)"
    "Llama 2",
    "Llama 3",
    "LoRA", # from "Low-Rank Adaptation (LoRA) / Parameter-Efficient Fine-Tuning (PEFT)"
    "Language Model Alignment",
    "Large Language Model(s)",
    "Large Multimodal Models (LMMs)",
    "Large Scale Training",
    "Low-Rank Adaptation (LoRA)",
    "MDP", # from "Markov Decision Process (MDP)"
    "MARL", # from "Multi-Agent Reinforcement Learning (MARL)"
    "Machine Learning",
    "Manipulation",
    "Markov Decision Process (MDP)",
    "Meta-Learning",
    "Meta-RL", # from "Meta-Reinforcement Learning (Meta-RL)"
    "Meta-Reinforcement Learning (Meta-RL)",
    "Metrics",
    "Mip-NeRF",
    "Mixture of Experts (MoE)",
    "Model Compression",
    "Model Inference",
    "Model-Based RL",
    "Model-Free RL",
    "Motion Planning",
    "Multimodal AI",
    "Multimodal Alignment",
    "Multimodal Fusion",
    "Multimodal LLMs",
    "Multimodal Learning",
    "Multi-Agent Reinforcement Learning (MARL)",
    "NPU (Neural Processing Unit)",
    "Navigation",
    "NeRF (Neural Radiance Fields)",
    "Network Compression",
    "Neural Network(s)",
    "Neural Rendering",
    "Neuromorphic Computing",
    "ONNX Runtime",
    "Offline Reinforcement Learning",
    "Optimization",
    "Overfitting",
    "PEFT", # from "Low-Rank Adaptation (LoRA) / Parameter-Efficient Fine-Tuning (PEFT)"
    "PIM", # from "In-Memory Computing / Processing-in-Memory (PIM)"
    "PLM(s)", # from "Pre-trained Language Model(s) / PLM(s)"
    "PPO", # from "Actor-Critic / A2C / A3C / PPO / SAC / DDPG"
    "PTQ", # from "Post-Training Quantization (PTQ)"
    "PaLM",
    "Parameter-Efficient Fine-Tuning (PEFT)",
    "Phenaki",
    "Planning",
    "Plenoxels",
    "Policy Gradient",
    "Policy Optimization",
    "Post-Training Quantization (PTQ)",
    "Pre-trained Language Model(s)",
    "Processing-in-Memory (PIM)",
    "Program Synthesis",
    "Prompt Design",
    "Prompt Engineering",
    "Pruning",
    "QAT", # from "Quantization-Aware Training (QAT)"
    "Q-Learning", # from "Value Function / Q-Learning / DQN"
    "Quantization",
    "Quantization-Aware Training (QAT)",
    "Question Answering (QA)",
    "RAG", # from "Knowledge Retrieval / Retrieval Augmented Generation (RAG)"
    "RL", # from "Reinforcement Learning / RL"
    "RL for LLM",
    "RLHF (Reinforcement Learning from Human Feedback)",
    "ROCm",
    "Reasoning",
    "Regularization",
    "Reinforcement Learning",
    "Reinforcement Learning for Robotics",
    "Reinforcement Learning with Language Models",
    "Representation Learning",
    "Responsible AI",
    "Retrieval Augmented Generation (RAG)",
    "Reward Design",
    "Reward Shaping",
    "RoBERTa", # from "BERT / RoBERTa / ALBERT"
    "Robot Control",
    "Robot Learning",
    "Robot Manipulation",
    "Robot Navigation",
    "Robotics",
    "Robustness",
    "SAC", # from "Actor-Critic / A2C / A3C / PPO / SAC / DDPG"
    "SGD", # from "Optimization / Stochastic Gradient Descent (SGD) / Adam"
    "SLAM (Simultaneous Localization and Mapping)",
    "SNN) Hardware", # from "Neuromorphic Computing / Spiking Neural Network (SNN) Hardware" -> might need manual fix for "Spiking Neural Network (SNN) Hardware"
    "SSL", # from "Self-Supervised Learning (SSL)"
    "Safety",
    "Scalable Training",
    "Scene Representation",
    "Self-Supervised Learning (SSL)",
    "Shape Generation",
    "Sim-to-Real",
    "Simulation for Robotics",
    "Sora",
    "Sparsity",
    "Speech Recognition",
    "Speech Synthesis",
    "Spiking Neural Network (SNN) Hardware",
    "Stable Diffusion",
    "Stochastic Gradient Descent (SGD)",
    "Summarization",
    "Systolic Array",
    "T5",
    "TPU (Tensor Processing Unit)",
    "TRL (Transformer Reinforcement Learning)", # Assuming TRL means this, it's an example of a more specific term
    "TVM",
    "Task Planning",
    "TensorRT",
    "Text-to-3D",
    "Text-to-Image Generation",
    "Text-to-Video Generation",
    "Tool Augmented LLMs",
    "Tool Use",
    "ToT (Tree of Thoughts)", # Assuming ToT means Tree of Thoughts
    "Trajectory Optimization",
    "Transfer Learning",
    "Transformer(s)",
    "Tree of Thoughts (ToT)",
    "Triton Inference Server",
    "V+L", # from "Vision-Language (VL / V+L) / Vision and Language"
    "VL", # from "Vision-Language (VL / V+L) / Vision and Language"
    "VQA", # from "Visual Question Answering (VQA) / Video QA"
    "Value Function",
    "Video Captioning",
    "Video QA",
    "View Synthesis",
    "Vision and Language",
    "Vision-Based Robotics",
    "Vision-Language (VL / V+L)",
    "Visual Question Answering (VQA)",
    "Visual Servoing",
    "Volumetric Rendering",
    "World Models",
    "Zero-Shot Learning",
]